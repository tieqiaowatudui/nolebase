

快速排序是通过选择一个元素,将整个数组分为两部分
- 大的
- 小的
同时确定被选中的位置.然后就继续对大小两个区间重复这一套,继续下去.直到还剩下一个元素.

# 时间复杂度
在理想状态下,每一次都均匀将整个数组分为两部分.这个时候可以看出这是一棵树.树高度是 O($log_2n$).然后每一次将整个区间分为两部分的时间复杂度是O(n).所以快排的时间复杂度是O(n$log_2n$).

# 遇到的问题
由于快排的时间复杂度理想情况是选中元素将整个数组一分为二.如果分得并不均匀,每次都是最边上.那么时间复杂度就会退化为 O(n^2).

# 如何解决
随机化,在选择元素的时候,可以随机选择一个元素,这样就可以大概率的保证将整个数组分割的竟可能均匀.

# 快速排序的速度
快速排序和堆排序以及归并排序在理论上的时间复杂度都是O(n$log_2n$).但是在工程上,快排是后两者的2~3倍.原因是中间的操作,快排比归并排序和堆排序要简单一些.

# 为什么快速排序比归并排序和堆排序要快2~3倍
主要有以下原因:
## 缓存命中率
快排的数据都是在一个数组中,连在一起的,所以高速缓存命中率高.而归并排序需要额外O(n)空间存放中间结果.而堆排序的父子切换是不连续的,可能会有缓存失效的情况.
# 逻辑简单
快排只是简单的比较和交换,而堆排序需要和两个孩子都进行比较,归并排序需要额外的比较.
## 指令流水线预测
因为快速排序只是简单的比较和交换,逻辑相对简单,更容易触发指令预测.而堆排序和归并排序可能要差一些.
综上所述,快排的快一方面是快排的中间逻辑相对简单,更重要的是快排算法设计更容易利用上cpu中的高速缓存和指令流水线预测的硬件加速的功能.
在工程上,短的数据使用插入排序,长的大规模数据使用快排.

## **4.7 分布式排序：通过分桶进行排序**

我们可以根据姓氏的首字母来划分电话簿中的姓名，将它们分成 26 个不同的堆，或称为桶（bucket）。注意到，任何落入 J 桶的名字必然排在 I 桶中所有名字之后、且排在 K 桶中所有名字之前。因此，我们可以分别对每个桶单独排序，最后只需将所有排好序的桶按顺序连接起来即可。

如果名字在桶之间分布均匀，那么这 26 个小排序问题将比原始问题小得多。进一步地，通过继续根据名字的第二个字母细分每个桶，我们能得到越来越小的桶。当每个桶中只剩一个名字时，排序就完成了。这种算法通常称为 **桶排序（bucketsort）或分布式排序（distribution sort）**。

只要我们确信数据分布大致均匀，分桶就是非常有效的想法。它也是哈希表、kd-tree 以及许多其他实用数据结构背后的基本思想。  
但缺点是：当数据分布与预期不符时，这类技术的性能可能非常糟糕。虽然像平衡二叉树这样的数据结构在任何输入分布下都能保证最坏情况的性能，但对于依赖启发式分布假设的数据结构，并没有这样的保证。

现实中确实存在不均匀分布。例如美国有一个罕见的姓——Shifflett。我上次查时，人口超过一百万的曼哈顿电话簿里只有五个 Shifflett。那么在一个人口 5 万的小城市中，应该有多少个 Shifflett 呢？图 4.8 展示了弗吉尼亚州夏洛茨维尔市电话簿中长达两页半的 Shifflett 名单。Shifflett 家族在当地非常庞大，但对于任何分布排序程序来说，这会造成严重的麻烦：从 S → Sh → Shi → Shif → …… → Shifflett 的桶划分几乎不会带来任何有效的分割。

**结论：**  
排序几乎可以展示所有主要的算法设计范式。数据结构技术、分治思想、随机化、增量构造等，都能导向高效的排序算法。
### **4.7.1 排序的下界**
关于排序复杂度，我们还需要讨论最后一个问题。我们已经看到若干在最坏情况下运行时间为 **O(n log n)** 的排序算法，但没有一个能达到线性时间。要对 n 个元素进行排序，显然至少要检查所有元素，因此任何排序算法在最坏情况下都必须需要 **Ω(n)** 时间。那么我们能否弥补剩下的 **Θ(log n)** 差距？  
答案是否定的。

一个 **Ω(n log n)** 的下界可以通过如下观察得到：对于 n 个键，它们有 **n! 种不同排列**，而排序算法在执行过程中必须对每一种排列表现出不同的行为。任何基于比较的排序算法，其执行过程都受成对比较的结果所控制。

我们可以把这种算法所有可能的执行过程想象成一棵有 **n! 个叶子** 的树。树的最小高度对应于可能的最快算法，而数学上我们知道：

> **lg(n!) = Θ(n log n)**

因此基于比较的排序算法在最坏情况下必然需要 **Ω(n log n)** 时间。

这个下界的重要性在于以下几点：  
首先，这个方法可以扩展，用来给许多依赖排序的应用提供下界，例如判断元素是否唯一、寻找众数、构造凸包等。排序问题是少数拥有非平凡下界的算法问题之一。我们将在第9章介绍另一种论证快速算法不太可能存在的方法。

## **4.8 战争故事：Skiena 出庭作证**

我过着安静、相当诚实的生活。这样的好处之一是，我通常不会突然接到律师的电话，更不会被律师主动找上门谈论排序算法。因此，当一位律师打电话给我，不仅想和我说话，还想和我讨论排序算法时，我相当惊讶。

原来，她所在的律所正在处理一个涉及高性能排序程序的案件，需要一位专家证人向陪审团解释相关技术问题。通过我这本书的第一版，他们意识到我懂一些算法，不过在聘请我之前，他们要求看我的教学评估，以证明我能够把事情讲清楚给别人听。  
这件事最终让我有机会深入了解“真正快速的排序程序”到底如何工作。我原以为可以借此机会终于弄清楚到底哪个原地排序算法在实践中最快：是堆排序还是快速排序？到底有哪些微妙而秘密的算法技巧在减少比较次数方面产生差异？  
答案令人谦卑——根本没人关心原地排序。重点是如何对巨大文件排序，这些文件远远无法放入主存。整个排序过程最关键的部分在于如何把数据从磁盘读入和写出。那些“可爱的”内存内部排序算法并不是重点，因为实际问题在于要同时处理几个GB的数据。

请记住，磁盘具有相对较长的寻道时间，也就是磁盘旋转到读写头下方所需的时间。一旦读写头到达正确位置，数据移动就相对很快，并且读取大块数据与读取单字节的成本差不多。因此目标是**最小化读取/写入的数据块数量**，并协调这些操作，确保排序算法永远不会因为等待数据而停顿。

排序中磁盘瓶颈最明显的体现，就是每年的 Minutesort 大赛。目标是在一分钟内排序尽可能多的数据。目前的冠军是 IBM Research 的 Jim Wyllie，他用一个由 40 个节点、80 颗 Itanium 处理器组成的小集群，以及一个包含 2,520 块磁盘的 SAN 阵列，在 58.7 秒内排序了 116 GB 的数据。  
稍微接地气的还有 Pennysort 分组，它追求的是“每一分钱硬件预算带来的最大排序性能”。目前的冠军（来自中国的 BSIS）在一台价值 760 美元、装有四块 SATA 硬盘的 PC 上，用 1,679 秒排序了 32 GB 的数据。你可以在 **SortBenchmark** 网站查看最新记录。

那么，哪种算法最适合外部排序？答案基本是**多路归并排序**，加上大量工程优化和专门技巧。

具体做法是：对每个已排好序的 k 个列表，取其顶部数据块中的元素建一个堆。不断从堆中取出最小（或最大）的元素，就能合并出一个有序序列。因为这个堆位于主存中，这些操作非常快。当你得到足够大的有序片段后，就把它写回磁盘，为更多数据腾出内存。一旦某个列表的顶部数据块元素快用完，就读入下一块。

在这一层级上对排序程序进行基准测试并决定谁更快，几乎是不可能的。比如：  
比较一个通用商业程序和一个专门为整数高度优化的轻量版本，这样公平吗？  
Minutesort 使用随机生成的 100 字节记录。这和排序姓名或整数完全是两个世界。例如，一种常见技巧是先截取键的一个较短前缀，先按这个前缀排序，避免搬运大量无关字节。

我们能从中学到什么？  
最重要的一点是：**无论如何，都要尽一切努力避免卷入诉讼，不论是当原告还是被告。**法院并不是快速解决争端的工具。法律战和军事战斗非常相似：升级很快、非常昂贵（时间、金钱、精力），并且往往只有在双方精疲力尽并妥协时才会结束。有智慧的人会在不上法院的情况下解决问题。现在理解这一点，能为你节省远超本书价格的成本。

从技术角度看，当你结合巨大的数据集与低复杂度算法（线性或 n log n）时，必须关注**外存性能**。即使常数因子只有 5 或 10，也可能决定一个方案是可行的还是完全无望的。当然，二次时间算法面对大规模数据，无论访问时间如何，都是注定失败的。